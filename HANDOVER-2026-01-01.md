# fcrawl Handover - 2026-01-01

## Session Summary

This session focused on improving the fcrawl CLI tool with better scraping and fixing search pagination.

---

## Completed Work

### 1. Metadata Header in Scrape Output
**Commit**: `cc1b87b` - feat(scrape): prepend metadata header to markdown output

Added Jina-style metadata header to scrape output:
```
Title: Page Title
URL Source: https://example.com
Published Time: 2025-07-31T08:07:07-05:00

Markdown Content:
[actual content]
```

**File**: `src/fcrawl/commands/scrape.py`
- Added `format_with_metadata()` function
- Extracts `title`, `source_url`/`url`, `published_time` from result.metadata

---

### 2. Article Mode & Selector Options
**Commit**: `412d822` - feat(scrape): add article mode and include/exclude options

New scrape options:

| Option | Description |
|--------|-------------|
| `--article` | Aggressive article extraction (like Jina reader) |
| `-i/--include` | CSS selectors to include (repeatable) |
| `-e/--exclude` | CSS selectors to exclude (repeatable) |
| `--raw` | Disable all filtering, get full page |

**How `--article` works**:
1. Sends aggressive `include_tags` and `exclude_tags` to Firecrawl API
2. Post-processes markdown with `clean_article_content()` to remove:
   - Share buttons (Twitter, Facebook, LinkedIn, Reddit)
   - Popup text ("Close", CTAs)
   - Newsletter prompts
   - Empty lines

**File**: `src/fcrawl/commands/scrape.py`
- Added `ARTICLE_INCLUDE_TAGS` and `ARTICLE_EXCLUDE_TAGS` constants
- Added `clean_article_content()` function with regex patterns
- Removed unused `--selector` and `--no-cache` options

---

### 3. SearXNG Pagination Fix
**NOT COMMITTED to fcrawl repo** - this is in the main Firecrawl API.

**Problem**: `fcrawl search -l 30` only returned ~10 results because SearXNG only fetched page 1.

**Fix**: Modified `apps/api/src/search/v2/searxng.ts` to loop through pages until limit is reached.

```typescript
// Added pagination loop
while (allResults.length < targetCount && page <= maxPages) {
  // fetch page
  // deduplicate by URL
  // add to results
  page++;
}
```

**To apply**: Rebuild Docker with `docker compose build api && docker compose up -d`

---

## Not Implemented: Caching Feature

### User's Vision
Local file-based caching to avoid re-scraping the same URLs:

```
/tmp/fcrawl-cache/
├── scrape/
│   ├── {url_hash}.md
│   ├── {url_hash}.html
│   └── {url_hash}.json
├── search/
│   └── {query_hash}.json
└── crawl/
    └── {url_hash}/
        ├── manifest.json
        ├── page_0.md
        └── ...
```

### Design Decisions Needed
1. **Cache location**: `/tmp/fcrawl-cache/` vs `~/.cache/fcrawl/`
2. **TTL**: How long before cache expires? (1 hour default?)
3. **Flags**:
   - `--no-cache` - skip reading cache, still write
   - `--refresh-cache` or `--force` - force refetch + update cache
   - `--cache-only` - only read from cache, don't fetch

### Implementation Plan
1. Create `utils/cache.py` with:
   - `get_cache_path(url, format)` - generate cache file path
   - `read_cache(url, format)` - read if exists and not expired
   - `write_cache(url, format, content)` - write to cache
   - `clear_cache()` - clear all cached files

2. Modify `scrape.py`:
   - Before API call: check cache
   - After API call: write to cache
   - Respect `--no-cache` and `--refresh-cache` flags

3. Modify `search.py`:
   - Cache search results by query hash
   - Include search options in hash (limit, sources, etc.)

4. Modify `crawl.py`:
   - Cache is trickier - multiple pages per crawl
   - Maybe cache individual pages, store manifest

### Cache Key Generation
```python
import hashlib

def cache_key(url: str, options: dict = None) -> str:
    """Generate cache key from URL and options"""
    key = url
    if options:
        key += json.dumps(options, sort_keys=True)
    return hashlib.sha256(key.encode()).hexdigest()[:16]
```

---

## Other Notes

### SearXNG Explained
SearXNG is a **meta-search engine** that queries multiple engines (Google, Bing, DDG) in parallel and aggregates results. It's self-hosted for privacy.

- User's SearXNG runs at `localhost:8888`
- Firecrawl API calls SearXNG when `SEARXNG_ENDPOINT` is set
- Results are deduplicated by URL across engines

### Firecrawl Selector Support
Firecrawl supports CSS selectors via:
- `include_tags` (Python SDK) / `includeTags` (API) - keep only these elements
- `exclude_tags` (Python SDK) / `excludeTags` (API) - remove these elements
- `only_main_content` - auto-remove nav/footer/sidebar (default: True)

### Commands Not Tested
User mentioned they'd test `crawl` and `map` commands later:
- `map` has hardcoded 50 URL display limit (not an issue, saves full list to file)
- `crawl` has hardcoded 10 page display limit (same)

---

## Files Modified This Session

| File | Changes |
|------|---------|
| `src/fcrawl/commands/scrape.py` | Metadata header, article mode, include/exclude, raw mode |
| `apps/api/src/search/v2/searxng.ts` | Pagination loop (Firecrawl API, not fcrawl) |

---

## Git Log

```
412d822 feat(scrape): add article mode and include/exclude options
cc1b87b feat(scrape): prepend metadata header to markdown output
```
